{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757659216026
        }
      },
      "outputs": [],
      "source": [
        "# Idea of ViT:\n",
        "'''\n",
        "\n",
        "\n",
        "1. Patch Embedding - this includes\n",
        "    1.1. Conv2d (passing each patch as 16x16 pixels) -> reshaping the patches (flatten)\n",
        "    1.2. Adding cls token [This is added to the no. of patches + cls token]\n",
        "            (Learnable parameter for the class i.e., like trying to put the image in a multi-dimentional map so it would be easier to classify)\n",
        "    1.2. adding positional embedding (This patch belongs to which/what position?)\n",
        "            (Learnable parameter where the positional embedding dimension = same as patch embedding dimension D)\n",
        "    Note: Each patch has its own positional embedding and 1 cls token is added to the front of the entire embedding along with a positional embedding\n",
        "2. Transformer Encoder\n",
        "    2.1. Multi-Head Self Attention (MHA)\n",
        "        2.1.1. converting to Q, K, V (Query, Key, Value)\n",
        "        2.1.2. Scaled Dot production Attention\n",
        "                2.1.2.1. Q,K -> MatMul -> Scale -> Mask -> SoftMax -> Out\n",
        "                2.1.2.2. Out, V -> MatMul ->FinOut\n",
        "        2.1.3. Concatenate\n",
        "        2.1.4. Linear Layer            \n",
        "    2.2. Transformer Block\n",
        "        2.2.1. Layer Norm\n",
        "        2.2.2 MLP\n",
        "            2.2.2.1. FC\n",
        "            2.2.2.2. GeLu\n",
        "            2.2.2.3. FC\n",
        "        2.2.3. Layer Norm\n",
        "3. MLP head\n",
        "    3.1. Take the cls token from the last output -> Linear layer (-> GeLu -> Dropout -> Linear )\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1757795342937
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as f\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1757795343094
        }
      },
      "outputs": [],
      "source": [
        "# Global Variables\n",
        "# I am intially setting these for MNIST Dataset\n",
        "IMG_SIZE = 28\n",
        "PATCH_SIZE = 4\n",
        "IN_CHANNELS = 1\n",
        "NUM_CLASSES = 10\n",
        "EMBED_DIM = 64\n",
        "TRANSFORMER_DEPTH = 6\n",
        "NUM_HEADS = 8\n",
        "MLP_RATIO = 4.0\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1757795343251
        }
      },
      "outputs": [],
      "source": [
        "# 1. Patch Embedding\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size=28, patch_size=4, in_channels=1, embed_dim=64 ):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size==0, \"Image size must be divisible by patch size\"\n",
        "        self.num_patches = (image_size//patch_size)**2\n",
        "\n",
        "        # using conv2d network to convert the images to patches with kernel_size=patch_size, stirde=patch_size for non-overlapping patches\n",
        "        self.proj = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.proj(x) # 1.1.\n",
        "        x = x.flatten(2) # 1.1.\n",
        "\n",
        "        # To transform (batch, embed_dim, num_patches) ->  (batch, num_patches, embed_dim)\n",
        "        x = x.transpose(1,2) \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1757795343419
        }
      },
      "outputs": [],
      "source": [
        "# 2. Transformer Encoder\n",
        "# 2.1. MultiHead self-ATTENTION\n",
        "class MHSA(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, qkv_bias=True, attention_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim*3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attention_drop)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "    \n",
        "    def forward(self, x, return_attention = False):\n",
        "        B, N, C = x.shape # Batch_size, N_tokens, Embedding_dim\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, B, heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # 2.1.2.1\n",
        "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # 2.1.2.2\n",
        "        out = (attn @ v)\n",
        "        out = out.transpose(1,2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        if return_attention:\n",
        "            return out, attn\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1757795343560
        }
      },
      "outputs": [],
      "source": [
        "# 2.2. Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, qkv_bias=True, p=0.0, attn_p=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.attn = MHSA(embed_dim, num_heads, qkv_bias, attention_drop=attn_p, proj_drop=p)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "        hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(p)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, return_attention=False):\n",
        "        if return_attention:\n",
        "            attn_out, attn = self.attn(self.norm1(x), return_attention)\n",
        "            x = x + attn_out\n",
        "            x = x + self.mlp(self.norm2(x))\n",
        "            return x, attn\n",
        "        else:\n",
        "            x = x + self.attn(self.norm1(x))\n",
        "            x = x + self.mlp(self.norm2(x))\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1757796508478
        }
      },
      "outputs": [],
      "source": [
        "# ViT\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=28, patch_size=4, in_channels=1, num_classes=10, embed_dim=64, depth=6, num_heads=8, mlp_ratio=4.0, p=0.0):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # CLS token + Positional Embeddings\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p)\n",
        "\n",
        "        # Transformer block\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, p=p, attn_p=p) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "\n",
        "        # Linear weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x, return_all_attention=False):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        attentions = []\n",
        "        if return_all_attention:\n",
        "            for blk in self.blocks:\n",
        "                x, attn = blk(x, return_attention=True)\n",
        "                attentions.append(attn)\n",
        "        else:\n",
        "            for blk in self.blocks:\n",
        "                x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls_final = x[:, 0]\n",
        "        logits = self.head(cls_final)\n",
        "        if return_all_attention:\n",
        "            return logits, attentions\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1757796509303
        }
      },
      "outputs": [],
      "source": [
        "def get_dataLoaders(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Downloading datasets\n",
        "    train_dataset = datasets.MNIST(root=\"..\", train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.MNIST(root=\"..\", train=False, transform=transform, download=True)\n",
        "\n",
        "    # Dataloader for NN\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1757796509570
        }
      },
      "outputs": [],
      "source": [
        "# Train & eval\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        # print(outputs.shape, labels.shape)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += imgs.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "    return running_loss / total, correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757796530216
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01  time 18.9s  train_loss 0.7376 train_acc 0.7454  val_loss 0.3529 val_acc 0.8857\n",
            "Best val acc: 0.8857\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    train_loader, test_loader = get_dataLoaders()\n",
        "    model = ViT(img_size=IMG_SIZE, patch_size=PATCH_SIZE, in_channels=IN_CHANNELS,\n",
        "                num_classes=NUM_CLASSES, embed_dim=EMBED_DIM, depth=TRANSFORMER_DEPTH, num_heads=NUM_HEADS, mlp_ratio=MLP_RATIO, p=DROPOUT).to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "        t1 = time.time()\n",
        "        print(f\"Epoch {epoch:02d}  time {(t1-t0):.1f}s  train_loss {train_loss:.4f} train_acc {train_acc:.4f}  val_loss {val_loss:.4f} val_acc {val_acc:.4f}\")\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            # torch.save(model.state_dict(), \"vit_mnist_best.pth\")\n",
        "        break\n",
        "    print(\"Best val acc:\", best_acc)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
